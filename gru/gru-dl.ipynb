{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10114984,"sourceType":"datasetVersion","datasetId":6240699},{"sourceId":10125021,"sourceType":"datasetVersion","datasetId":6248006}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torch import nn\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport joblib\nfrom torch.autograd import Variable\nimport gensim, logging\nimport gensim.downloader as api\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.metrics import classification_report\n","metadata":{"id":"mbHk-0I5fDwM","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T01:25:45.460496Z","iopub.execute_input":"2024-12-09T01:25:45.460955Z","iopub.status.idle":"2024-12-09T01:25:56.531513Z","shell.execute_reply.started":"2024-12-09T01:25:45.460899Z","shell.execute_reply":"2024-12-09T01:25:56.530796Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"trainloader =  joblib.load('/kaggle/input/final-dl/train_data_loader.pkl')\ntestloader = joblib.load('/kaggle/input/final-dl/test_data_loader.pkl')\n\nBATCH_SIZE = 128\ntrain_dataloader = torch.utils.data.DataLoader(trainloader.dataset,batch_size= BATCH_SIZE, shuffle= True)\ntest_dataloader = torch.utils.data.DataLoader(testloader.dataset,batch_size= BATCH_SIZE)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T01:25:56.533231Z","iopub.execute_input":"2024-12-09T01:25:56.534547Z","iopub.status.idle":"2024-12-09T01:25:57.268438Z","shell.execute_reply.started":"2024-12-09T01:25:56.534516Z","shell.execute_reply":"2024-12-09T01:25:57.267435Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(io.BytesIO(b))\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from torch import nn\nfrom tqdm import tqdm\nimport torch.nn.functional as F\n#--------------------------------------------training loop----------------------\n#we will output F1 score or confusion matrix at each step\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\n\ndef F1_tensor(y_true, y_pred):\n    y_true = y_true.to('cpu').numpy()\n    y_pred = y_pred.to('cpu').numpy()\n    return f1_score(y_true, y_pred)\n\ndef Confusion_matrix_tensor(y_true, y_pred):\n    y_true = y_true.to('cpu').numpy()\n    y_pred = y_pred.to('cpu').numpy()\n    return f1_score(y_true, y_pred)\n\ndef convert_from_tensor(y): #convert from tensor to some kind of array that we can use numpy\n    return y.cpu().detach().numpy().reshape(-1)\n\ndef take_all_elem(container, target):\n    for x in target:\n        if (x != 0 and x != 1):\n            container.append(1)\n        else:\n            container.append(x)\n\ndef save_model(model):\n    MODEL_PATH = Path('/kaggle/working/')\n    MODEL_PATH.mkdir(parents = True, exist_ok = True)\n    MODEL_NAME = 'best_LSTMmodel.pth'\n    MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n    print(f'Update new best model to : {MODEL_SAVE_PATH}')\n    torch.save(obj = model.state_dict(),f = MODEL_SAVE_PATH)\n\ndef train_step(model : nn.Module,\n               data_loader : torch.utils.data.DataLoader,\n               loss_function : nn.Module,\n               optimizer,\n               device = 'cuda'):\n    model.train()\n    loss = 0\n\n    all_y_true = []\n    all_y_pred = []\n\n    for batch in data_loader:\n        X_train = batch[0].to(device)\n        y_train = batch[1].to(device)\n        #print(y_train.type())\n        y_pred = model(X_train)\n        y_pred01 = torch.round(torch.sigmoid(y_pred))\n\n        batch_loss = loss_function(y_pred.float(),y_train.unsqueeze(1).float())\n        #print(batch_loss.type())\n        loss += batch_loss\n\n        take_all_elem(all_y_true,convert_from_tensor(y_train))\n        take_all_elem(all_y_pred,convert_from_tensor(y_pred01))\n\n\n        optimizer.zero_grad()\n        batch_loss.backward()\n        optimizer.step()\n\n    loss /= len(data_loader)\n\n    all_y_true = np.array(all_y_true)\n    all_y_pred = np.array(all_y_pred)\n\n    #print(all_y_true)\n    #print(np.unique(all_y_true))\n\n    print('------------------Train Result----------------------------')\n    print(f'Training loss : {loss} | F1_score : {f1_score(all_y_true,all_y_pred)}')\n    print(f'Confusion matrix :')\n    print(confusion_matrix(all_y_true,all_y_pred))\n    print(f'Classification report :')\n    print(classification_report(all_y_true, all_y_pred, digits=4))\n\n\ndef test_step(model : nn.Module,\n              data_loader : torch.utils.data.DataLoader,\n              loss_function : nn.Module,\n              optimizer,\n              device = 'cuda'):\n\n    model.eval()\n    loss,acc = 0,0\n    all_y_true = []\n    all_y_pred = []\n\n    with torch.no_grad():\n        loss = 0\n\n        for (X_test,y_test) in data_loader:\n            X_test = X_test.to(device)\n            y_test = y_test.to(device)\n\n            test_logits = model(X_test).squeeze()\n            test_01 = torch.round(torch.sigmoid(test_logits))\n\n            batch_loss = loss_function(test_logits.float(),y_test.float())\n\n            loss += batch_loss\n\n            take_all_elem(all_y_true,convert_from_tensor(y_test))\n            take_all_elem(all_y_pred,convert_from_tensor(test_01))\n\n\n        loss /= len(data_loader)\n        acc /= len(data_loader)\n    current_f1_score = f1_score(all_y_true,all_y_pred)\n    print('------------------Test Result----------------------------')\n    print(f'Testing loss : {loss} | F1_score : {current_f1_score}')\n    global best_f1_score\n    if (current_f1_score > best_f1_score):\n        best_f1_score = current_f1_score\n        save_model(model)\n    print(f'Confusion matrix :')\n    print(confusion_matrix(all_y_true,all_y_pred))\n    print(f'Classification report :')\n    print(classification_report(all_y_true, all_y_pred, digits=4))\n    print('---------------------------------------------------------')\n\nmatrix_size = (64001,768)\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim):\n        super(Attention, self).__init__()\n        self.attention = nn.Linear(hidden_dim, 1)\n\n    def forward(self, lstm_output):\n        \"\"\"\n        lstm_output: Tensor of shape (batch_size, seq_len, hidden_dim)\n        \"\"\"\n        # Compute attention scores\n        attn_scores = self.attention(lstm_output).squeeze(-1)  # (batch_size, seq_len)\n        attn_weights = F.softmax(attn_scores, dim=1)  # Normalize scores to probabilities\n        \n        # Compute context vector as weighted sum of LSTM outputs\n        context = torch.bmm(attn_weights.unsqueeze(1), lstm_output).squeeze(1)  # (batch_size, hidden_dim)\n        \n        return context, attn_weights\n\nclass LSTMmodel(nn.Module):\n    def __init__(self, matrix_size):\n        super().__init__()\n        self.embedding = nn.Embedding(64001,768)\n        self.rnn = nn.LSTM(input_size=matrix_size[1], hidden_size=512,\n                           num_layers=2, batch_first=True, bidirectional=True)\n        self.attention = Attention(512 * 2)\n        # Output size should be doubled due to bidirectionality\n        self.fc = nn.Linear(512 * 2,out_features=1)\n\n    def forward(self, X):\n        embedding = self.embedding(X)\n        lstm_outputs, _ = self.rnn(embedding)\n        context, attn_weights = self.attention(lstm_outputs)\n        #print(rnn_outputs.shape)\n        #if len(rnn_outputs.shape) == 2:  # When seq_len = 1, rnn_outputs has shape [batch_size, hidden_size * 2]\n         #   rnn_outputs = rnn_outputs.unsqueeze(1)  # Add a dimension to match expected shape\n\n        # Select the last hidden state of the sequence for each batch\n        #last_hidden_state = rnn_outputs[:, -1, :]\n        # Pass it through the final fully connected layer\n        return self.fc(context)\n    \ndevice = 'cuda'\nmodel = LSTMmodel(matrix_size).to(device)\nBCE_loss = nn.BCEWithLogitsLoss()\nAdam_optimizer = torch.optim.AdamW(params = model.parameters(),lr = 0.001)\n\n\nepochs = 5\nbest_f1_score = -1\nfor epoch in tqdm(range(0,epochs)):\n    print(f'Epoch {epoch}=======================================')\n    train_step(model,train_dataloader,BCE_loss,Adam_optimizer,device = device)\n    test_step(model,test_dataloader,BCE_loss,Adam_optimizer,device = device)\n    #break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:09:50.629739Z","iopub.execute_input":"2024-12-09T02:09:50.630332Z","iopub.status.idle":"2024-12-09T02:16:03.891588Z","shell.execute_reply.started":"2024-12-09T02:09:50.630300Z","shell.execute_reply":"2024-12-09T02:16:03.890682Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/5 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 0=======================================\n------------------Train Result----------------------------\nTraining loss : 0.4057580530643463 | F1_score : 0.6022588684447744\nConfusion matrix :\n[[29991  1810]\n [ 5691  5679]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8405    0.9431    0.8888     31801\n           1     0.7583    0.4995    0.6023     11370\n\n    accuracy                         0.8262     43171\n   macro avg     0.7994    0.7213    0.7456     43171\nweighted avg     0.8189    0.8262    0.8134     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.35370075702667236 | F1_score : 0.6723926380368097\nUpdate new best model to : /kaggle/working/best_LSTMmodel.pth\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 1/5 [01:14<04:58, 74.51s/it]","output_type":"stream"},{"name":"stdout","text":"Confusion matrix :\n[[3759  193]\n [ 608  822]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8608    0.9512    0.9037      3952\n           1     0.8099    0.5748    0.6724      1430\n\n    accuracy                         0.8512      5382\n   macro avg     0.8353    0.7630    0.7881      5382\nweighted avg     0.8472    0.8512    0.8423      5382\n\n---------------------------------------------------------\nEpoch 1=======================================\n------------------Train Result----------------------------\nTraining loss : 0.27972298860549927 | F1_score : 0.7647726198354922\nConfusion matrix :\n[[30106  1695]\n [ 3281  8089]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.9017    0.9467    0.9237     31801\n           1     0.8268    0.7114    0.7648     11370\n\n    accuracy                         0.8847     43171\n   macro avg     0.8642    0.8291    0.8442     43171\nweighted avg     0.8820    0.8847    0.8818     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.33841362595558167 | F1_score : 0.729352878083661\nUpdate new best model to : /kaggle/working/best_LSTMmodel.pth\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 2/5 [02:29<03:44, 74.91s/it]","output_type":"stream"},{"name":"stdout","text":"Confusion matrix :\n[[3605  347]\n [ 410 1020]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8979    0.9122    0.9050      3952\n           1     0.7462    0.7133    0.7294      1430\n\n    accuracy                         0.8593      5382\n   macro avg     0.8220    0.8127    0.8172      5382\nweighted avg     0.8576    0.8593    0.8583      5382\n\n---------------------------------------------------------\nEpoch 2=======================================\n------------------Train Result----------------------------\nTraining loss : 0.1733601987361908 | F1_score : 0.8641114982578396\nConfusion matrix :\n[[30620  1181]\n [ 1822  9548]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.9438    0.9629    0.9533     31801\n           1     0.8899    0.8398    0.8641     11370\n\n    accuracy                         0.9304     43171\n   macro avg     0.9169    0.9013    0.9087     43171\nweighted avg     0.9296    0.9304    0.9298     43171\n\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 3/5 [03:43<02:29, 74.63s/it]","output_type":"stream"},{"name":"stdout","text":"------------------Test Result----------------------------\nTesting loss : 0.4196995496749878 | F1_score : 0.7102129195236375\nConfusion matrix :\n[[3595  357]\n [ 446  984]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8896    0.9097    0.8995      3952\n           1     0.7338    0.6881    0.7102      1430\n\n    accuracy                         0.8508      5382\n   macro avg     0.8117    0.7989    0.8049      5382\nweighted avg     0.8482    0.8508    0.8492      5382\n\n---------------------------------------------------------\nEpoch 3=======================================\n------------------Train Result----------------------------\nTraining loss : 0.0909842923283577 | F1_score : 0.9323388385721897\nConfusion matrix :\n[[31147   654]\n [  870 10500]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.9728    0.9794    0.9761     31801\n           1     0.9414    0.9235    0.9323     11370\n\n    accuracy                         0.9647     43171\n   macro avg     0.9571    0.9515    0.9542     43171\nweighted avg     0.9645    0.9647    0.9646     43171\n\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 4/5 [04:58<01:14, 74.49s/it]","output_type":"stream"},{"name":"stdout","text":"------------------Test Result----------------------------\nTesting loss : 0.4988979995250702 | F1_score : 0.7005967005967007\nConfusion matrix :\n[[3531  421]\n [ 432  998]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8910    0.8935    0.8922      3952\n           1     0.7033    0.6979    0.7006      1430\n\n    accuracy                         0.8415      5382\n   macro avg     0.7972    0.7957    0.7964      5382\nweighted avg     0.8411    0.8415    0.8413      5382\n\n---------------------------------------------------------\nEpoch 4=======================================\n------------------Train Result----------------------------\nTraining loss : 0.04765291139483452 | F1_score : 0.9684573246156565\nConfusion matrix :\n[[31496   305]\n [  409 10961]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.9872    0.9904    0.9888     31801\n           1     0.9729    0.9640    0.9685     11370\n\n    accuracy                         0.9835     43171\n   macro avg     0.9801    0.9772    0.9786     43171\nweighted avg     0.9834    0.9835    0.9834     43171\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5/5 [06:12<00:00, 74.51s/it]","output_type":"stream"},{"name":"stdout","text":"------------------Test Result----------------------------\nTesting loss : 0.6509868502616882 | F1_score : 0.699437148217636\nConfusion matrix :\n[[3649  303]\n [ 498  932]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8799    0.9233    0.9011      3952\n           1     0.7547    0.6517    0.6994      1430\n\n    accuracy                         0.8512      5382\n   macro avg     0.8173    0.7875    0.8003      5382\nweighted avg     0.8466    0.8512    0.8475      5382\n\n---------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\n\ndef F1_tensor(y_true, y_pred):\n    y_true = y_true.to('cpu').numpy()\n    y_pred = y_pred.to('cpu').numpy()\n    return f1_score(y_true, y_pred)\n\ndef Confusion_matrix_tensor(y_true, y_pred):\n    y_true = y_true.to('cpu').numpy()\n    y_pred = y_pred.to('cpu').numpy()\n    return f1_score(y_true, y_pred)\n\ndef convert_from_tensor(y): #convert from tensor to some kind of array that we can use numpy\n    return y.cpu().detach().numpy().reshape(-1)\n\ndef take_all_elem(container, target):\n    for x in target:\n        if (x != 0 and x != 1):\n            container.append(1)\n        else:\n            container.append(x)\n\ndef save_model(model):\n    MODEL_PATH = Path('/content')\n    MODEL_PATH.mkdir(parents = True, exist_ok = True)\n    MODEL_NAME = 'best_GRUmodel.pth'\n    MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n    print(f'Update new best model to : {MODEL_SAVE_PATH}')\n    torch.save(obj = model.state_dict(),f = MODEL_SAVE_PATH)\n\n","metadata":{"id":"hy5vE5fPO5-m","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:08:25.476622Z","iopub.execute_input":"2024-12-09T02:08:25.477475Z","iopub.status.idle":"2024-12-09T02:08:25.484147Z","shell.execute_reply.started":"2024-12-09T02:08:25.477437Z","shell.execute_reply":"2024-12-09T02:08:25.483304Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def train_step(model : nn.Module,\n               data_loader : torch.utils.data.DataLoader,\n               loss_function : nn.Module,\n               optimizer,\n               device = 'cuda'):\n    model.train()\n    loss = 0\n\n    all_y_true = []\n    all_y_pred = []\n\n    for (X_train,y_train) in data_loader:\n        X_train = X_train.to(device)\n        y_train = y_train.unsqueeze(1).to(device)\n\n        y_pred = model(X_train)\n        y_pred01 = torch.round(torch.sigmoid(y_pred))\n\n        batch_loss = loss_function(y_pred.float(),y_train.float())\n        loss += batch_loss\n\n        take_all_elem(all_y_true,convert_from_tensor(y_train))\n        take_all_elem(all_y_pred,convert_from_tensor(y_pred01))\n\n\n        optimizer.zero_grad()\n        batch_loss.backward()\n        optimizer.step()\n\n    loss /= len(data_loader)\n\n    all_y_true = np.array(all_y_true)\n    all_y_pred = np.array(all_y_pred)\n\n    #print(all_y_true)\n    #print(np.unique(all_y_true))\n\n    print('------------------Train Result----------------------------')\n    print(f'Training loss : {loss} | F1_score : {f1_score(all_y_true,all_y_pred)}')\n\n    print(classification_report(all_y_true, all_y_pred, digits=4))\n\nbest_f1_score = -1\n","metadata":{"id":"2JlNwITjPH9W","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:23:33.210285Z","iopub.execute_input":"2024-12-08T10:23:33.210540Z","iopub.status.idle":"2024-12-08T10:23:33.224222Z","shell.execute_reply.started":"2024-12-08T10:23:33.210515Z","shell.execute_reply":"2024-12-08T10:23:33.223434Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def test_step(model : nn.Module,\n              data_loader : torch.utils.data.DataLoader,\n              loss_function : nn.Module,\n              optimizer,\n              device = 'cuda',):\n\n    model.eval()\n    loss,acc = 0,0\n\n    all_y_true = []\n    all_y_pred = []\n\n    with torch.inference_mode():\n        loss = 0\n\n        for (X_test,y_test) in data_loader:\n            X_test = X_test.to(device)\n            y_test = y_test.to(device)\n\n            test_logits = model(X_test).squeeze()\n            test_01 = torch.round(torch.sigmoid(test_logits))\n\n            batch_loss = loss_function(test_logits.float(),y_test.float())\n\n            loss += batch_loss\n\n            take_all_elem(all_y_true,convert_from_tensor(y_test))\n            take_all_elem(all_y_pred,convert_from_tensor(test_01))\n\n\n        loss /= len(data_loader)\n        acc /= len(data_loader)\n\n    current_f1_score = f1_score(all_y_true,all_y_pred)\n    print('------------------Test Result----------------------------')\n    print(f'Testing loss : {loss} | F1_score : {current_f1_score}')\n    print('---------------------------------------------------------')\n\n    print(classification_report(all_y_true, all_y_pred, digits=4))\n\n    global best_f1_score\n    if (current_f1_score > best_f1_score):\n        best_f1_score = current_f1_score\n        save_model(model)\n\nmatrix_size = (128,768)\n\n\n","metadata":{"id":"iu8piAnkPNa2","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:23:33.225145Z","iopub.execute_input":"2024-12-08T10:23:33.225394Z","iopub.status.idle":"2024-12-08T10:23:33.239035Z","shell.execute_reply.started":"2024-12-08T10:23:33.225355Z","shell.execute_reply":"2024-12-08T10:23:33.238340Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class GRUmodel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.embedding = nn.Embedding(64001, 768)\n        self.rnn = nn.GRU(input_size = matrix_size[1],hidden_size = 384,\n            num_layers = 1, batch_first = True, bidirectional = False)\n\n        self.fc = nn.LazyLinear(out_features = 1)\n\n    #it output [0,1,2,....,seq_length - 1]\n    #just take the last array element in case of classification or anything like that\n    def forward(self, X, state=None):\n        X = self.embedding(X)\n        rnn_outputs, _ = self.rnn(X, state)\n\n        return self.fc(rnn_outputs)[:, -1, :]\n\n    def feature_extract(self, X, state = None):\n        rnn_outputs, _ = self.rnn(X, state)\n        return rnn_outputs[:,-1,:]\n","metadata":{"id":"fzdMDVtAPQqp","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:23:33.240816Z","iopub.execute_input":"2024-12-08T10:23:33.241094Z","iopub.status.idle":"2024-12-08T10:23:33.251986Z","shell.execute_reply.started":"2024-12-08T10:23:33.241070Z","shell.execute_reply":"2024-12-08T10:23:33.251240Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel = GRUmodel().to(device)\nBCE_loss = nn.BCEWithLogitsLoss()\nAdam_optimizer = torch.optim.AdamW(params = model.parameters(),lr = 0.0001)\n","metadata":{"id":"Ps5B2GbBPS3G","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:23:33.252805Z","iopub.execute_input":"2024-12-08T10:23:33.253058Z","iopub.status.idle":"2024-12-08T10:23:34.763112Z","shell.execute_reply.started":"2024-12-08T10:23:33.253034Z","shell.execute_reply":"2024-12-08T10:23:34.762433Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"epochs = 30\n\nfor epoch in range(0,epochs):\n    print(f'Epoch {epoch}=======================================')\n    train_step(model,train_dataloader,BCE_loss,Adam_optimizer,device = device)\n    test_step( model,test_dataloader,BCE_loss,Adam_optimizer,device = device)\n    ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OTyy5lf3PUal","outputId":"834762b1-31af-4025-b5e7-74d6093a59d3","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:23:34.764106Z","iopub.execute_input":"2024-12-08T10:23:34.764553Z","iopub.status.idle":"2024-12-08T10:34:33.100025Z","shell.execute_reply.started":"2024-12-08T10:23:34.764525Z","shell.execute_reply":"2024-12-08T10:34:33.099014Z"}},"outputs":[{"name":"stdout","text":"Epoch 0=======================================\n------------------Train Result----------------------------\nTraining loss : 0.5795738697052002 | F1_score : 0.006955916876793323\n              precision    recall  f1-score   support\n\n           0     0.7368    0.9971    0.8474     31801\n           1     0.3053    0.0035    0.0070     11370\n\n    accuracy                         0.7354     43171\n   macro avg     0.5211    0.5003    0.4272     43171\nweighted avg     0.6231    0.7354    0.6260     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.5805929899215698 | F1_score : 0.0\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.7343    1.0000    0.8468      3952\n           1     0.0000    0.0000    0.0000      1430\n\n    accuracy                         0.7343      5382\n   macro avg     0.3671    0.5000    0.4234      5382\nweighted avg     0.5392    0.7343    0.6218      5382\n\nUpdate new best model to : /content/best_GRUmodel.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1=======================================\n------------------Train Result----------------------------\nTraining loss : 0.521623969078064 | F1_score : 0.26818181818181813\n              precision    recall  f1-score   support\n\n           0     0.7657    0.9742    0.8574     31801\n           1     0.6967    0.1661    0.2682     11370\n\n    accuracy                         0.7613     43171\n   macro avg     0.7312    0.5701    0.5628     43171\nweighted avg     0.7475    0.7613    0.7022     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.4302860498428345 | F1_score : 0.5929810407422348\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8396    0.9205    0.8782      3952\n           1     0.7007    0.5140    0.5930      1430\n\n    accuracy                         0.8125      5382\n   macro avg     0.7701    0.7173    0.7356      5382\nweighted avg     0.8027    0.8125    0.8024      5382\n\nUpdate new best model to : /content/best_GRUmodel.pth\nEpoch 2=======================================\n------------------Train Result----------------------------\nTraining loss : 0.3802271783351898 | F1_score : 0.6573259800268296\n              precision    recall  f1-score   support\n\n           0     0.8618    0.9326    0.8958     31801\n           1     0.7554    0.5818    0.6573     11370\n\n    accuracy                         0.8402     43171\n   macro avg     0.8086    0.7572    0.7766     43171\nweighted avg     0.8338    0.8402    0.8330     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.3852260410785675 | F1_score : 0.6744098913450731\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8721    0.9142    0.8926      3952\n           1     0.7264    0.6294    0.6744      1430\n\n    accuracy                         0.8385      5382\n   macro avg     0.7992    0.7718    0.7835      5382\nweighted avg     0.8334    0.8385    0.8347      5382\n\nUpdate new best model to : /content/best_GRUmodel.pth\nEpoch 3=======================================\n------------------Train Result----------------------------\nTraining loss : 0.31855231523513794 | F1_score : 0.7369785415966588\n              precision    recall  f1-score   support\n\n           0     0.8904    0.9439    0.9164     31801\n           1     0.8113    0.6751    0.7370     11370\n\n    accuracy                         0.8731     43171\n   macro avg     0.8509    0.8095    0.8267     43171\nweighted avg     0.8696    0.8731    0.8691     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.3723974823951721 | F1_score : 0.6879354602126879\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8796    0.9092    0.8941      3952\n           1     0.7232    0.6559    0.6879      1430\n\n    accuracy                         0.8419      5382\n   macro avg     0.8014    0.7826    0.7910      5382\nweighted avg     0.8380    0.8419    0.8393      5382\n\nUpdate new best model to : /content/best_GRUmodel.pth\nEpoch 4=======================================\n------------------Train Result----------------------------\nTraining loss : 0.26981955766677856 | F1_score : 0.7891162790697674\n              precision    recall  f1-score   support\n\n           0     0.9126    0.9482    0.9301     31801\n           1     0.8374    0.7461    0.7891     11370\n\n    accuracy                         0.8950     43171\n   macro avg     0.8750    0.8471    0.8596     43171\nweighted avg     0.8928    0.8950    0.8930     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.390051007270813 | F1_score : 0.6753990994678674\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8615    0.9524    0.9047      3952\n           1     0.8144    0.5769    0.6754      1430\n\n    accuracy                         0.8527      5382\n   macro avg     0.8380    0.7647    0.7900      5382\nweighted avg     0.8490    0.8527    0.8438      5382\n\nEpoch 5=======================================\n------------------Train Result----------------------------\nTraining loss : 0.226176455616951 | F1_score : 0.8322992700729928\n              precision    recall  f1-score   support\n\n           0     0.9311    0.9551    0.9429     31801\n           1     0.8646    0.8023    0.8323     11370\n\n    accuracy                         0.9149     43171\n   macro avg     0.8979    0.8787    0.8876     43171\nweighted avg     0.9136    0.9149    0.9138     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.39123642444610596 | F1_score : 0.6941979522184302\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8936    0.8778    0.8856      3952\n           1     0.6780    0.7112    0.6942      1430\n\n    accuracy                         0.8335      5382\n   macro avg     0.7858    0.7945    0.7899      5382\nweighted avg     0.8363    0.8335    0.8348      5382\n\nUpdate new best model to : /content/best_GRUmodel.pth\nEpoch 6=======================================\n------------------Train Result----------------------------\nTraining loss : 0.1865275651216507 | F1_score : 0.8691979330487531\n              precision    recall  f1-score   support\n\n           0     0.9474    0.9619    0.9546     31801\n           1     0.8886    0.8507    0.8692     11370\n\n    accuracy                         0.9326     43171\n   macro avg     0.9180    0.9063    0.9119     43171\nweighted avg     0.9319    0.9326    0.9321     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.3976408839225769 | F1_score : 0.7010309278350516\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8949    0.8836    0.8892      3952\n           1     0.6892    0.7133    0.7010      1430\n\n    accuracy                         0.8384      5382\n   macro avg     0.7921    0.7984    0.7951      5382\nweighted avg     0.8403    0.8384    0.8392      5382\n\nUpdate new best model to : /content/best_GRUmodel.pth\nEpoch 7=======================================\n------------------Train Result----------------------------\nTraining loss : 0.15134666860103607 | F1_score : 0.9008776228449236\n              precision    recall  f1-score   support\n\n           0     0.9608    0.9696    0.9652     31801\n           1     0.9128    0.8893    0.9009     11370\n\n    accuracy                         0.9485     43171\n   macro avg     0.9368    0.9294    0.9330     43171\nweighted avg     0.9481    0.9485    0.9482     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.41788238286972046 | F1_score : 0.700394689630427\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8872    0.9036    0.8953      3952\n           1     0.7192    0.6825    0.7004      1430\n\n    accuracy                         0.8449      5382\n   macro avg     0.8032    0.7931    0.7979      5382\nweighted avg     0.8426    0.8449    0.8435      5382\n\nEpoch 8=======================================\n------------------Train Result----------------------------\nTraining loss : 0.11843909323215485 | F1_score : 0.9257614776662818\n              precision    recall  f1-score   support\n\n           0     0.9705    0.9771    0.9738     31801\n           1     0.9348    0.9169    0.9258     11370\n\n    accuracy                         0.9613     43171\n   macro avg     0.9526    0.9470    0.9498     43171\nweighted avg     0.9611    0.9613    0.9611     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.4485996663570404 | F1_score : 0.7025114962858153\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8903    0.8978    0.8940      3952\n           1     0.7108    0.6944    0.7025      1430\n\n    accuracy                         0.8437      5382\n   macro avg     0.8006    0.7961    0.7983      5382\nweighted avg     0.8426    0.8437    0.8432      5382\n\nUpdate new best model to : /content/best_GRUmodel.pth\nEpoch 9=======================================\n------------------Train Result----------------------------\nTraining loss : 0.09710744023323059 | F1_score : 0.9421692069803401\n              precision    recall  f1-score   support\n\n           0     0.9778    0.9811    0.9795     31801\n           1     0.9466    0.9378    0.9422     11370\n\n    accuracy                         0.9697     43171\n   macro avg     0.9622    0.9594    0.9608     43171\nweighted avg     0.9696    0.9697    0.9696     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.5268495082855225 | F1_score : 0.6970041714069017\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8776    0.9271    0.9017      3952\n           1     0.7614    0.6427    0.6970      1430\n\n    accuracy                         0.8515      5382\n   macro avg     0.8195    0.7849    0.7993      5382\nweighted avg     0.8467    0.8515    0.8473      5382\n\nEpoch 10=======================================\n------------------Train Result----------------------------\nTraining loss : 0.07437077164649963 | F1_score : 0.9574665429972175\n              precision    recall  f1-score   support\n\n           0     0.9834    0.9864    0.9849     31801\n           1     0.9617    0.9533    0.9575     11370\n\n    accuracy                         0.9777     43171\n   macro avg     0.9725    0.9699    0.9712     43171\nweighted avg     0.9776    0.9777    0.9777     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.5803331136703491 | F1_score : 0.6923886355664679\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8882    0.8902    0.8892      3952\n           1     0.6946    0.6902    0.6924      1430\n\n    accuracy                         0.8370      5382\n   macro avg     0.7914    0.7902    0.7908      5382\nweighted avg     0.8367    0.8370    0.8369      5382\n\nEpoch 11=======================================\n------------------Train Result----------------------------\nTraining loss : 0.06354174017906189 | F1_score : 0.9639607808497482\n              precision    recall  f1-score   support\n\n           0     0.9857    0.9887    0.9872     31801\n           1     0.9682    0.9598    0.9640     11370\n\n    accuracy                         0.9811     43171\n   macro avg     0.9769    0.9743    0.9756     43171\nweighted avg     0.9811    0.9811    0.9811     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.6131740808486938 | F1_score : 0.6891638545181852\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8767    0.9195    0.8976      3952\n           1     0.7429    0.6427    0.6892      1430\n\n    accuracy                         0.8460      5382\n   macro avg     0.8098    0.7811    0.7934      5382\nweighted avg     0.8412    0.8460    0.8422      5382\n\nEpoch 12=======================================\n------------------Train Result----------------------------\nTraining loss : 0.05356798693537712 | F1_score : 0.9707112970711298\n              precision    recall  f1-score   support\n\n           0     0.9890    0.9901    0.9896     31801\n           1     0.9722    0.9692    0.9707     11370\n\n    accuracy                         0.9846     43171\n   macro avg     0.9806    0.9797    0.9801     43171\nweighted avg     0.9846    0.9846    0.9846     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.5683557391166687 | F1_score : 0.6951526032315979\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8853    0.9021    0.8936      3952\n           1     0.7144    0.6769    0.6952      1430\n\n    accuracy                         0.8423      5382\n   macro avg     0.7998    0.7895    0.7944      5382\nweighted avg     0.8399    0.8423    0.8409      5382\n\nEpoch 13=======================================\n------------------Train Result----------------------------\nTraining loss : 0.04270438477396965 | F1_score : 0.9780925567481964\n              precision    recall  f1-score   support\n\n           0     0.9920    0.9923    0.9922     31801\n           1     0.9784    0.9777    0.9781     11370\n\n    accuracy                         0.9885     43171\n   macro avg     0.9852    0.9850    0.9851     43171\nweighted avg     0.9885    0.9885    0.9885     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.6437910199165344 | F1_score : 0.6892617449664429\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8948    0.8677    0.8810      3952\n           1     0.6626    0.7182    0.6893      1430\n\n    accuracy                         0.8279      5382\n   macro avg     0.7787    0.7929    0.7851      5382\nweighted avg     0.8331    0.8279    0.8301      5382\n\nEpoch 14=======================================\n------------------Train Result----------------------------\nTraining loss : 0.03986772894859314 | F1_score : 0.9788394703268664\n              precision    recall  f1-score   support\n\n           0     0.9923    0.9926    0.9924     31801\n           1     0.9792    0.9785    0.9788     11370\n\n    accuracy                         0.9889     43171\n   macro avg     0.9858    0.9855    0.9856     43171\nweighted avg     0.9889    0.9889    0.9889     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.6987801790237427 | F1_score : 0.6993603411513859\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8884    0.8988    0.8936      3952\n           1     0.7110    0.6881    0.6994      1430\n\n    accuracy                         0.8428      5382\n   macro avg     0.7997    0.7934    0.7965      5382\nweighted avg     0.8413    0.8428    0.8420      5382\n\nEpoch 15=======================================\n------------------Train Result----------------------------\nTraining loss : 0.03253103047609329 | F1_score : 0.9832490657287316\n              precision    recall  f1-score   support\n\n           0     0.9941    0.9939    0.9940     31801\n           1     0.9830    0.9835    0.9832     11370\n\n    accuracy                         0.9912     43171\n   macro avg     0.9886    0.9887    0.9886     43171\nweighted avg     0.9912    0.9912    0.9912     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.6897047758102417 | F1_score : 0.6891548784911135\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8816    0.9046    0.8930      3952\n           1     0.7159    0.6643    0.6892      1430\n\n    accuracy                         0.8408      5382\n   macro avg     0.7988    0.7845    0.7911      5382\nweighted avg     0.8376    0.8408    0.8388      5382\n\nEpoch 16=======================================\n------------------Train Result----------------------------\nTraining loss : 0.029530106112360954 | F1_score : 0.9847858587635213\n              precision    recall  f1-score   support\n\n           0     0.9946    0.9945    0.9946     31801\n           1     0.9847    0.9849    0.9848     11370\n\n    accuracy                         0.9920     43171\n   macro avg     0.9896    0.9897    0.9897     43171\nweighted avg     0.9920    0.9920    0.9920     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.7489510178565979 | F1_score : 0.690071725179313\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8761    0.9228    0.8988      3952\n           1     0.7498    0.6392    0.6901      1430\n\n    accuracy                         0.8475      5382\n   macro avg     0.8129    0.7810    0.7945      5382\nweighted avg     0.8425    0.8475    0.8434      5382\n\nEpoch 17=======================================\n------------------Train Result----------------------------\nTraining loss : 0.02776831015944481 | F1_score : 0.9858758305099662\n              precision    recall  f1-score   support\n\n           0     0.9948    0.9952    0.9950     31801\n           1     0.9864    0.9853    0.9859     11370\n\n    accuracy                         0.9926     43171\n   macro avg     0.9906    0.9902    0.9904     43171\nweighted avg     0.9926    0.9926    0.9926     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.7343045473098755 | F1_score : 0.6937521338340731\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8934    0.8778    0.8855      3952\n           1     0.6778    0.7105    0.6938      1430\n\n    accuracy                         0.8333      5382\n   macro avg     0.7856    0.7941    0.7896      5382\nweighted avg     0.8361    0.8333    0.8346      5382\n\nEpoch 18=======================================\n------------------Train Result----------------------------\nTraining loss : 0.02512456476688385 | F1_score : 0.9868131868131869\n              precision    recall  f1-score   support\n\n           0     0.9954    0.9951    0.9953     31801\n           1     0.9864    0.9872    0.9868     11370\n\n    accuracy                         0.9931     43171\n   macro avg     0.9909    0.9912    0.9910     43171\nweighted avg     0.9931    0.9931    0.9931     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.7991316914558411 | F1_score : 0.6839729119638827\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8746    0.9193    0.8964      3952\n           1     0.7402    0.6357    0.6840      1430\n\n    accuracy                         0.8439      5382\n   macro avg     0.8074    0.7775    0.7902      5382\nweighted avg     0.8389    0.8439    0.8399      5382\n\nEpoch 19=======================================\n------------------Train Result----------------------------\nTraining loss : 0.02190091274678707 | F1_score : 0.9880884356731572\n              precision    recall  f1-score   support\n\n           0     0.9959    0.9956    0.9957     31801\n           1     0.9876    0.9886    0.9881     11370\n\n    accuracy                         0.9937     43171\n   macro avg     0.9918    0.9921    0.9919     43171\nweighted avg     0.9937    0.9937    0.9937     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.7559796571731567 | F1_score : 0.6904411764705882\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8800    0.9112    0.8953      3952\n           1     0.7279    0.6566    0.6904      1430\n\n    accuracy                         0.8436      5382\n   macro avg     0.8040    0.7839    0.7929      5382\nweighted avg     0.8396    0.8436    0.8409      5382\n\nEpoch 20=======================================\n------------------Train Result----------------------------\nTraining loss : 0.022396830841898918 | F1_score : 0.9877238526862321\n              precision    recall  f1-score   support\n\n           0     0.9954    0.9958    0.9956     31801\n           1     0.9883    0.9872    0.9877     11370\n\n    accuracy                         0.9935     43171\n   macro avg     0.9918    0.9915    0.9917     43171\nweighted avg     0.9935    0.9935    0.9935     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.7612428069114685 | F1_score : 0.6910175177040627\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8782    0.9175    0.8974      3952\n           1     0.7398    0.6483    0.6910      1430\n\n    accuracy                         0.8460      5382\n   macro avg     0.8090    0.7829    0.7942      5382\nweighted avg     0.8414    0.8460    0.8426      5382\n\nEpoch 21=======================================\n------------------Train Result----------------------------\nTraining loss : 0.01869131065905094 | F1_score : 0.9894412670479543\n              precision    recall  f1-score   support\n\n           0     0.9961    0.9964    0.9962     31801\n           1     0.9899    0.9890    0.9894     11370\n\n    accuracy                         0.9944     43171\n   macro avg     0.9930    0.9927    0.9928     43171\nweighted avg     0.9944    0.9944    0.9944     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.8309771418571472 | F1_score : 0.6927775781530722\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8843    0.9016    0.8929      3952\n           1     0.7125    0.6741    0.6928      1430\n\n    accuracy                         0.8411      5382\n   macro avg     0.7984    0.7878    0.7928      5382\nweighted avg     0.8387    0.8411    0.8397      5382\n\nEpoch 22=======================================\n------------------Train Result----------------------------\nTraining loss : 0.01905868574976921 | F1_score : 0.9896635144051023\n              precision    recall  f1-score   support\n\n           0     0.9962    0.9964    0.9963     31801\n           1     0.9899    0.9894    0.9897     11370\n\n    accuracy                         0.9946     43171\n   macro avg     0.9931    0.9929    0.9930     43171\nweighted avg     0.9946    0.9946    0.9946     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.7917361855506897 | F1_score : 0.690756941802967\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8752    0.9264    0.9001      3952\n           1     0.7573    0.6350    0.6908      1430\n\n    accuracy                         0.8489      5382\n   macro avg     0.8163    0.7807    0.7954      5382\nweighted avg     0.8439    0.8489    0.8444      5382\n\nEpoch 23=======================================\n------------------Train Result----------------------------\nTraining loss : 0.019540637731552124 | F1_score : 0.9886513591976775\n              precision    recall  f1-score   support\n\n           0     0.9958    0.9960    0.9959     31801\n           1     0.9889    0.9884    0.9887     11370\n\n    accuracy                         0.9940     43171\n   macro avg     0.9924    0.9922    0.9923     43171\nweighted avg     0.9940    0.9940    0.9940     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.811733067035675 | F1_score : 0.6850174216027874\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8866    0.8844    0.8855      3952\n           1     0.6826    0.6874    0.6850      1430\n\n    accuracy                         0.8320      5382\n   macro avg     0.7846    0.7859    0.7853      5382\nweighted avg     0.8324    0.8320    0.8322      5382\n\nEpoch 24=======================================\n------------------Train Result----------------------------\nTraining loss : 0.016307994723320007 | F1_score : 0.9910258666197431\n              precision    recall  f1-score   support\n\n           0     0.9967    0.9969    0.9968     31801\n           1     0.9914    0.9907    0.9910     11370\n\n    accuracy                         0.9953     43171\n   macro avg     0.9940    0.9938    0.9939     43171\nweighted avg     0.9953    0.9953    0.9953     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.8379407525062561 | F1_score : 0.6897851488880512\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8762    0.9221    0.8985      3952\n           1     0.7482    0.6399    0.6898      1430\n\n    accuracy                         0.8471      5382\n   macro avg     0.8122    0.7810    0.7942      5382\nweighted avg     0.8422    0.8471    0.8431      5382\n\nEpoch 25=======================================\n------------------Train Result----------------------------\nTraining loss : 0.016613073647022247 | F1_score : 0.9901486498372768\n              precision    recall  f1-score   support\n\n           0     0.9964    0.9965    0.9965     31801\n           1     0.9902    0.9901    0.9901     11370\n\n    accuracy                         0.9948     43171\n   macro avg     0.9933    0.9933    0.9933     43171\nweighted avg     0.9948    0.9948    0.9948     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.8824855089187622 | F1_score : 0.6880357808423407\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8772    0.9165    0.8964      3952\n           1     0.7366    0.6455    0.6880      1430\n\n    accuracy                         0.8445      5382\n   macro avg     0.8069    0.7810    0.7922      5382\nweighted avg     0.8399    0.8445    0.8411      5382\n\nEpoch 26=======================================\n------------------Train Result----------------------------\nTraining loss : 0.012927886098623276 | F1_score : 0.9928298068886641\n              precision    recall  f1-score   support\n\n           0     0.9973    0.9975    0.9974     31801\n           1     0.9931    0.9925    0.9928     11370\n\n    accuracy                         0.9962     43171\n   macro avg     0.9952    0.9950    0.9951     43171\nweighted avg     0.9962    0.9962    0.9962     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.8866450190544128 | F1_score : 0.6882232811436352\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8918    0.8742    0.8830      3952\n           1     0.6704    0.7070    0.6882      1430\n\n    accuracy                         0.8298      5382\n   macro avg     0.7811    0.7906    0.7856      5382\nweighted avg     0.8330    0.8298    0.8312      5382\n\nEpoch 27=======================================\n------------------Train Result----------------------------\nTraining loss : 0.012556306086480618 | F1_score : 0.9930973840404484\n              precision    recall  f1-score   support\n\n           0     0.9976    0.9975    0.9975     31801\n           1     0.9929    0.9933    0.9931     11370\n\n    accuracy                         0.9964     43171\n   macro avg     0.9952    0.9954    0.9953     43171\nweighted avg     0.9964    0.9964    0.9964     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.85060715675354 | F1_score : 0.6945054945054945\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8819    0.9109    0.8962      3952\n           1     0.7292    0.6629    0.6945      1430\n\n    accuracy                         0.8450      5382\n   macro avg     0.8056    0.7869    0.7953      5382\nweighted avg     0.8414    0.8450    0.8426      5382\n\nEpoch 28=======================================\n------------------Train Result----------------------------\nTraining loss : 0.012571302242577076 | F1_score : 0.9920823436262866\n              precision    recall  f1-score   support\n\n           0     0.9971    0.9973    0.9972     31801\n           1     0.9923    0.9918    0.9921     11370\n\n    accuracy                         0.9958     43171\n   macro avg     0.9947    0.9945    0.9946     43171\nweighted avg     0.9958    0.9958    0.9958     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.9514963626861572 | F1_score : 0.6711462450592885\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8643    0.9365    0.8990      3952\n           1     0.7718    0.5937    0.6711      1430\n\n    accuracy                         0.8454      5382\n   macro avg     0.8181    0.7651    0.7851      5382\nweighted avg     0.8397    0.8454    0.8384      5382\n\nEpoch 29=======================================\n------------------Train Result----------------------------\nTraining loss : 0.01806146278977394 | F1_score : 0.9891734882492739\n              precision    recall  f1-score   support\n\n           0     0.9959    0.9964    0.9961     31801\n           1     0.9900    0.9884    0.9892     11370\n\n    accuracy                         0.9943     43171\n   macro avg     0.9929    0.9924    0.9927     43171\nweighted avg     0.9943    0.9943    0.9943     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.9160148501396179 | F1_score : 0.6773211567732116\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8709    0.9221    0.8958      3952\n           1     0.7429    0.6224    0.6773      1430\n\n    accuracy                         0.8424      5382\n   macro avg     0.8069    0.7722    0.7865      5382\nweighted avg     0.8369    0.8424    0.8377      5382\n\n","output_type":"stream"}],"execution_count":9}]}