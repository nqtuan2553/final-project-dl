{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10114984,"sourceType":"datasetVersion","datasetId":6240699},{"sourceId":10167760,"sourceType":"datasetVersion","datasetId":6279124}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torch import nn\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport joblib\nfrom torch.autograd import Variable\nimport gensim, logging\nimport gensim.downloader as api\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.metrics import classification_report\n\ntrainloader =  joblib.load('/kaggle/input/final-dl/train_data_loader.pkl',weights_only=True)\nvalloader = joblib.load('/kaggle/input/validation/val_data_loader.pkl',weights_only=True)\ntestloader = joblib.load('/kaggle/input/final-dl/test_data_loader.pkl',weights_only=True)\n\nBATCH_SIZE = 128\ntrain_dataloader = torch.utils.data.DataLoader(trainloader.dataset,batch_size= BATCH_SIZE, shuffle= True)\ntest_dataloader = torch.utils.data.DataLoader(testloader.dataset,batch_size= BATCH_SIZE)\nval_dataloader = torch.utils.data.DataLoader(valloader.dataset,batch_size= BATCH_SIZE)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch import nn\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\n\ndef F1_tensor(y_true, y_pred):\n    y_true = y_true.to('cpu').numpy()\n    y_pred = y_pred.to('cpu').numpy()\n    return f1_score(y_true, y_pred)\n\ndef Confusion_matrix_tensor(y_true, y_pred):\n    y_true = y_true.to('cpu').numpy()\n    y_pred = y_pred.to('cpu').numpy()\n    return f1_score(y_true, y_pred)\n\ndef convert_from_tensor(y): #convert from tensor to some kind of array that we can use numpy\n    return y.cpu().detach().numpy().reshape(-1)\n\ndef take_all_elem(container, target):\n    for x in target:\n        if (x != 0 and x != 1):\n            container.append(1)\n        else:\n            container.append(x)\n\ndef save_model(model):\n    MODEL_PATH = Path('/kaggle/working/')\n    MODEL_PATH.mkdir(parents = True, exist_ok = True)\n    MODEL_NAME = 'best_RNNmodel.pth'\n    MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n    print(f'Update new best model to : {MODEL_SAVE_PATH}')\n    torch.save(obj = model.state_dict(),f = MODEL_SAVE_PATH)\n\ndef train_step(model : nn.Module,\n               data_loader : torch.utils.data.DataLoader,\n               loss_function : nn.Module,\n               optimizer,\n               device = 'cuda'):\n    model.train()\n    loss = 0\n\n    all_y_true = []\n    all_y_pred = []\n\n    for (X_train,y_train) in data_loader:\n        X_train = X_train.to(device)\n        y_train = y_train.unsqueeze(1).to(device)\n\n        y_pred = model(X_train)\n        y_pred01 = torch.round(torch.sigmoid(y_pred))\n\n        batch_loss = loss_function(y_pred.float(),y_train.float())\n        loss += batch_loss\n\n        take_all_elem(all_y_true,convert_from_tensor(y_train))\n        take_all_elem(all_y_pred,convert_from_tensor(y_pred01))\n\n\n        optimizer.zero_grad()\n        batch_loss.backward()\n        optimizer.step()\n\n    loss /= len(data_loader)\n\n    all_y_true = np.array(all_y_true)\n    all_y_pred = np.array(all_y_pred)\n\n    #print(all_y_true)\n    #print(np.unique(all_y_true))\n\n    print('------------------Train Result----------------------------')\n    print(f'Training loss : {loss} | F1_score : {f1_score(all_y_true,all_y_pred)}')\n    print(f'Confusion matrix :')\n    print(confusion_matrix(all_y_true,all_y_pred))\n    print(f'Classification report :')\n    print(classification_report(all_y_true, all_y_pred, digits=4))\n    \ndef test_step(model : nn.Module,\n              data_loader : torch.utils.data.DataLoader,\n              loss_function : nn.Module,\n              optimizer,\n              device = 'cuda'):\n\n    model.eval()\n    loss,acc = 0,0\n\n    all_y_true = []\n    all_y_pred = []\n\n    with torch.inference_mode():\n        loss = 0\n\n        for (X_test,y_test) in data_loader:\n            X_test = X_test.to(device)\n            y_test = y_test.to(device)\n\n            test_logits = model(X_test).squeeze()\n            test_01 = torch.round(torch.sigmoid(test_logits))\n\n            batch_loss = loss_function(test_logits.float(),y_test.float())\n\n            loss += batch_loss\n\n            take_all_elem(all_y_true,convert_from_tensor(y_test))\n            take_all_elem(all_y_pred,convert_from_tensor(test_01))\n\n\n        loss /= len(data_loader)\n        acc /= len(data_loader)\n    current_f1_score = f1_score(all_y_true,all_y_pred)\n    print('------------------Test Result----------------------------')\n    print(f'Testing loss : {loss} | F1_score : {f1_score(all_y_true,all_y_pred)}')\n    global best_f1_score\n    if (current_f1_score > best_f1_score):\n        best_f1_score = current_f1_score\n        save_model(model)\n    print(f'Confusion matrix :')\n    print(confusion_matrix(all_y_true,all_y_pred))\n    print(f'Classification report :')\n    print(classification_report(all_y_true, all_y_pred, digits=4))\n    print('---------------------------------------------------------')\n\nmatrix_size = (64001,768)\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim):\n        super(Attention, self).__init__()\n        self.attention = nn.Linear(hidden_dim, 1)\n\n    def forward(self, lstm_output):\n        \"\"\"\n        lstm_output: Tensor of shape (batch_size, seq_len, hidden_dim)\n        \"\"\"\n        # Compute attention scores\n        attn_scores = self.attention(lstm_output).squeeze(-1)  # (batch_size, seq_len)\n        attn_weights = F.softmax(attn_scores, dim=1)  # Normalize scores to probabilities\n        \n        # Compute context vector as weighted sum of LSTM outputs\n        context = torch.bmm(attn_weights.unsqueeze(1), lstm_output).squeeze(1)  # (batch_size, hidden_dim)\n        \n        return context, attn_weights\n\nclass attRNNmodel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.embedding = nn.Embedding(64001,matrix_size[1])\n        self.rnn = nn.RNN(input_size = matrix_size[1],hidden_size = 32,\n            num_layers = 3, batch_first = True, bidirectional = True)\n        self.attention = Attention(32 * 2)\n        self.fc = nn.LazyLinear(out_features = 1)\n\n    #it output [0,1,2,....,seq_length - 1]\n    #just take the last array element in case of classification or anything like that\n    def forward(self, X, state=None):\n        embedding = self.embedding(X)\n        rnn_outputs, _ = self.rnn(embedding)\n        context, attn_weights = self.attention(rnn_outputs)\n        return self.fc(context)\n        \nclass RNNmodel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.embedding = nn.Embedding(64001,matrix_size[1])\n        self.rnn = nn.RNN(input_size = matrix_size[1],hidden_size = 32,\n            num_layers = 3, batch_first = True, bidirectional = True)\n        self.fc = nn.LazyLinear(out_features = 1)\n\n    #it output [0,1,2,....,seq_length - 1]\n    #just take the last array element in case of classification or anything like that\n    def forward(self, X, state=None):\n        embedding = self.embedding(X)\n        rnn_outputs, _ = self.rnn(embedding)\n        #context, attn_weights = self.attention(rnn_outputs)\n        return self.fc(rnn_outputs)[:, -1, :]\n\ndevice = 'cuda'\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = attRNNmodel().to(device)\nBCE_loss = nn.BCEWithLogitsLoss()\nAdam_optimizer = torch.optim.Adam(params = model.parameters(),lr = 0.001)\n\n\nepochs = 10\nbest_f1_score = -1\n\nfor epoch in range(0,epochs):\n    print(f'Epoch {epoch}=======================================')\n    train_step(model,train_dataloader,BCE_loss,Adam_optimizer,device = device)\n    test_step(model,val_dataloader,BCE_loss,Adam_optimizer,device = device)\n    #break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = RNNmodel().to(device)\nBCE_loss = nn.BCEWithLogitsLoss()\nAdam_optimizer = torch.optim.Adam(params = model.parameters(),lr = 0.001)\n\n\nepochs = 10\nbest_f1_score = -1\n\nfor epoch in range(0,epochs):\n    print(f'Epoch {epoch}=======================================')\n    train_step(model,train_dataloader,BCE_loss,Adam_optimizer,device = device)\n    test_step(model,val_dataloader,BCE_loss,Adam_optimizer,device = device)\n    #break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = RNNmodel().to(device)\nmodel.load_state_dict(torch.load('best_RNNmodel.pth'))\ntest_step(model,test_dataloader,BCE_loss,Adam_optimizer,device = device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:02:46.118372Z","iopub.status.idle":"2024-12-11T10:02:46.118667Z","shell.execute_reply.started":"2024-12-11T10:02:46.118529Z","shell.execute_reply":"2024-12-11T10:02:46.118544Z"}},"outputs":[],"execution_count":null}]}