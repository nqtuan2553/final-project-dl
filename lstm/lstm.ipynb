{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10125021,"sourceType":"datasetVersion","datasetId":6248006},{"sourceId":10170154,"sourceType":"datasetVersion","datasetId":6280957}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torch import nn\nimport torch\nimport numpy as np\nimport pandas as pd\nimport joblib\nfrom torch.autograd import Variable\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.metrics import classification_report\n","metadata":{"id":"mbHk-0I5fDwM","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:59:09.057775Z","iopub.execute_input":"2024-12-13T15:59:09.058672Z","iopub.status.idle":"2024-12-13T15:59:13.041386Z","shell.execute_reply.started":"2024-12-13T15:59:09.058623Z","shell.execute_reply":"2024-12-13T15:59:13.040427Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"trainloader =  joblib.load('/kaggle/input/data-loader/train_data_loader.pkl')\nvalloader = joblib.load('/kaggle/input/data-loader/val_data_loader.pkl')\ntestloader = joblib.load('/kaggle/input/data-loader/test_data_loader.pkl')\n\nBATCH_SIZE = 128\ntrain_dataloader = torch.utils.data.DataLoader(trainloader.dataset,batch_size= BATCH_SIZE, shuffle= True)\ntest_dataloader = torch.utils.data.DataLoader(testloader.dataset,batch_size= BATCH_SIZE)\nval_dataloader = torch.utils.data.DataLoader(valloader.dataset,batch_size= BATCH_SIZE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:59:13.043219Z","iopub.execute_input":"2024-12-13T15:59:13.043687Z","iopub.status.idle":"2024-12-13T15:59:14.301978Z","shell.execute_reply.started":"2024-12-13T15:59:13.043644Z","shell.execute_reply":"2024-12-13T15:59:14.301286Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(io.BytesIO(b))\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base-v2')\nphoBert = AutoModel.from_pretrained('vinai/phobert-base-v2')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch import nn\nfrom tqdm import tqdm\nimport torch.nn.functional as F\n#--------------------------------------------training loop----------------------\n#we will output F1 score or confusion matrix at each step\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\n\ndef F1_tensor(y_true, y_pred):\n    y_true = y_true.to('cpu').numpy()\n    y_pred = y_pred.to('cpu').numpy()\n    return f1_score(y_true, y_pred)\n\ndef Confusion_matrix_tensor(y_true, y_pred):\n    y_true = y_true.to('cpu').numpy()\n    y_pred = y_pred.to('cpu').numpy()\n    return f1_score(y_true, y_pred)\n\ndef convert_from_tensor(y): #convert from tensor to some kind of array that we can use numpy\n    return y.cpu().detach().numpy().reshape(-1)\n\ndef take_all_elem(container, target):\n    for x in target:\n        if (x != 0 and x != 1):\n            container.append(1)\n        else:\n            container.append(x)\n\ndef save_model(model):\n    MODEL_PATH = Path('/kaggle/working/')\n    MODEL_PATH.mkdir(parents = True, exist_ok = True)\n    MODEL_NAME = 'best_LSTMmodel.pth'\n    MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n    print(f'Update new best model to : {MODEL_SAVE_PATH}')\n    torch.save(obj = model.state_dict(),f = MODEL_SAVE_PATH)\n\ndef train_step(model : nn.Module,\n               data_loader : torch.utils.data.DataLoader,\n               loss_function : nn.Module,\n               optimizer,\n               device = 'cuda'):\n    model.train()\n    loss = 0\n\n    all_y_true = []\n    all_y_pred = []\n\n    for batch in data_loader:\n        X_train = batch[0].to(device)\n        y_train = batch[1].to(device)\n        mask = batch[2].to(device)\n        #print(y_train.type())\n        #embedding = phoBert(X_train,mask)[0]\n        y_pred = model(X_train)\n        y_pred01 = torch.round(torch.sigmoid(y_pred))\n\n        batch_loss = loss_function(y_pred.float(),y_train.unsqueeze(1).float())\n        #print(batch_loss.type())\n        loss += batch_loss\n\n        take_all_elem(all_y_true,convert_from_tensor(y_train))\n        take_all_elem(all_y_pred,convert_from_tensor(y_pred01))\n\n\n        optimizer.zero_grad()\n        batch_loss.backward()\n        optimizer.step()\n\n    loss /= len(data_loader)\n\n    all_y_true = np.array(all_y_true)\n    all_y_pred = np.array(all_y_pred)\n\n    #print(all_y_true)\n    #print(np.unique(all_y_true))\n\n    print('------------------Train Result----------------------------')\n    print(f'Training loss : {loss} | F1_score : {f1_score(all_y_true,all_y_pred)}')\n    print(f'Confusion matrix :')\n    print(confusion_matrix(all_y_true,all_y_pred))\n    print(f'Classification report :')\n    print(classification_report(all_y_true, all_y_pred, digits=4))\n\n\ndef test_step(model : nn.Module,\n              data_loader : torch.utils.data.DataLoader,\n              loss_function : nn.Module,\n              optimizer,\n              device = 'cuda'):\n\n    model.eval()\n    loss,acc = 0,0\n    all_y_true = []\n    all_y_pred = []\n\n    with torch.no_grad():\n        loss = 0\n\n        for (X_test,y_test,mask) in data_loader:\n            X_test = X_test.to(device)\n            y_test = y_test.to(device)\n            mask = mask.to(device)\n            #embedding = phoBert(X_test,mask)[0]\n            test_logits = model(X_test).squeeze()\n            test_01 = torch.round(torch.sigmoid(test_logits))\n\n            batch_loss = loss_function(test_logits.float(),y_test.float())\n\n            loss += batch_loss\n\n            take_all_elem(all_y_true,convert_from_tensor(y_test))\n            take_all_elem(all_y_pred,convert_from_tensor(test_01))\n\n\n        loss /= len(data_loader)\n        acc /= len(data_loader)\n    current_f1_score = f1_score(all_y_true,all_y_pred)\n    print('------------------Test Result----------------------------')\n    print(f'Testing loss : {loss} | F1_score : {current_f1_score}')\n    global best_f1_score\n    if (current_f1_score > best_f1_score):\n        best_f1_score = current_f1_score\n        save_model(model)\n    print(f'Confusion matrix :')\n    print(confusion_matrix(all_y_true,all_y_pred))\n    print(f'Classification report :')\n    print(classification_report(all_y_true, all_y_pred, digits=4))\n    print('---------------------------------------------------------')\n\nmatrix_size = (64001,768)\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim):\n        super(Attention, self).__init__()\n        self.attention = nn.Linear(hidden_dim, 1)\n\n    def forward(self, lstm_output):\n        \"\"\"\n        lstm_output: Tensor of shape (batch_size, seq_len, hidden_dim)\n        \"\"\"\n        # Compute attention scores\n        attn_scores = self.attention(lstm_output).squeeze(-1)  # (batch_size, seq_len)\n        attn_weights = F.softmax(attn_scores, dim=1)  # Normalize scores to probabilities\n        \n        # Compute context vector as weighted sum of LSTM outputs\n        context = torch.bmm(attn_weights.unsqueeze(1), lstm_output).squeeze(1)  # (batch_size, hidden_dim)\n        \n        return context, attn_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:59:47.480365Z","iopub.execute_input":"2024-12-13T15:59:47.481159Z","iopub.status.idle":"2024-12-13T15:59:47.497450Z","shell.execute_reply.started":"2024-12-13T15:59:47.481122Z","shell.execute_reply":"2024-12-13T15:59:47.496491Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class LSTMmodel(nn.Module):\n    def __init__(self, matrix_size):\n        super().__init__()\n        self.embedding = nn.Embedding(64001,768)\n        #phoBert\n        self.rnn = nn.LSTM(input_size=matrix_size[1], hidden_size=512,\n                           num_layers=2, batch_first=True, bidirectional=True)\n        self.attention = Attention(512 * 2)\n        # Output size should be doubled due to bidirectionality\n        self.fc = nn.Linear(512 * 2,out_features=1)\n\n    def forward(self,X):\n        embedding = self.embedding(X)\n        lstm_outputs, _ = self.rnn(embedding)\n        context, attn_weights = self.attention(lstm_outputs)\n        return self.fc(context)\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:59:57.306340Z","iopub.execute_input":"2024-12-13T15:59:57.306668Z","iopub.status.idle":"2024-12-13T15:59:57.312362Z","shell.execute_reply.started":"2024-12-13T15:59:57.306640Z","shell.execute_reply":"2024-12-13T15:59:57.311510Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device = 'cuda'\nmodel = LSTMmodel(matrix_size).to(device)\nBCE_loss = nn.BCEWithLogitsLoss()\nAdam_optimizer = torch.optim.AdamW(params = model.parameters(),lr = 0.001)\n\n\nepochs = 10\nbest_f1_score = -1\nfor epoch in tqdm(range(0,epochs)):\n    print(f'Epoch {epoch}=======================================')\n    train_step(model,train_dataloader,BCE_loss,Adam_optimizer,device = device)\n    test_step(model,val_dataloader,BCE_loss,Adam_optimizer,device = device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T16:00:01.520836Z","iopub.execute_input":"2024-12-13T16:00:01.521633Z","iopub.status.idle":"2024-12-13T16:12:19.536461Z","shell.execute_reply.started":"2024-12-13T16:00:01.521596Z","shell.execute_reply":"2024-12-13T16:12:19.535559Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/10 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 0=======================================\n------------------Train Result----------------------------\nTraining loss : 0.4027549624443054 | F1_score : 0.6110020688557637\nConfusion matrix :\n[[30079  1722]\n [ 5611  5759]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8428    0.9459    0.8913     31801\n           1     0.7698    0.5065    0.6110     11370\n\n    accuracy                         0.8301     43171\n   macro avg     0.8063    0.7262    0.7512     43171\nweighted avg     0.8236    0.8301    0.8175     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.36150553822517395 | F1_score : 0.6809353943717796\nUpdate new best model to : /kaggle/working/best_LSTMmodel.pth\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 1/10 [01:13<11:00, 73.38s/it]","output_type":"stream"},{"name":"stdout","text":"Confusion matrix :\n[[3718  220]\n [ 585  859]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8640    0.9441    0.9023      3938\n           1     0.7961    0.5949    0.6809      1444\n\n    accuracy                         0.8504      5382\n   macro avg     0.8301    0.7695    0.7916      5382\nweighted avg     0.8458    0.8504    0.8429      5382\n\n---------------------------------------------------------\nEpoch 1=======================================\n------------------Train Result----------------------------\nTraining loss : 0.28000539541244507 | F1_score : 0.7661244562133536\nConfusion matrix :\n[[30124  1677]\n [ 3269  8101]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.9021    0.9473    0.9241     31801\n           1     0.8285    0.7125    0.7661     11370\n\n    accuracy                         0.8854     43171\n   macro avg     0.8653    0.8299    0.8451     43171\nweighted avg     0.8827    0.8854    0.8825     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.3568698465824127 | F1_score : 0.7041198501872659\nUpdate new best model to : /kaggle/working/best_LSTMmodel.pth\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 2/10 [02:27<09:51, 73.90s/it]","output_type":"stream"},{"name":"stdout","text":"Confusion matrix :\n[[3652  286]\n [ 504  940]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8787    0.9274    0.9024      3938\n           1     0.7667    0.6510    0.7041      1444\n\n    accuracy                         0.8532      5382\n   macro avg     0.8227    0.7892    0.8033      5382\nweighted avg     0.8487    0.8532    0.8492      5382\n\n---------------------------------------------------------\nEpoch 2=======================================\n------------------Train Result----------------------------\nTraining loss : 0.17630010843276978 | F1_score : 0.8643547367470423\nConfusion matrix :\n[[30596  1205]\n [ 1799  9571]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.9445    0.9621    0.9532     31801\n           1     0.8882    0.8418    0.8644     11370\n\n    accuracy                         0.9304     43171\n   macro avg     0.9163    0.9019    0.9088     43171\nweighted avg     0.9296    0.9304    0.9298     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.4107227623462677 | F1_score : 0.7048671039005868\nUpdate new best model to : /kaggle/working/best_LSTMmodel.pth\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 3/10 [03:41<08:38, 74.03s/it]","output_type":"stream"},{"name":"stdout","text":"Confusion matrix :\n[[3506  432]\n [ 423 1021]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8923    0.8903    0.8913      3938\n           1     0.7027    0.7071    0.7049      1444\n\n    accuracy                         0.8411      5382\n   macro avg     0.7975    0.7987    0.7981      5382\nweighted avg     0.8415    0.8411    0.8413      5382\n\n---------------------------------------------------------\nEpoch 3=======================================\n------------------Train Result----------------------------\nTraining loss : 0.08572310954332352 | F1_score : 0.9381918002302311\nConfusion matrix :\n[[31180   621]\n [  775 10595]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.9757    0.9805    0.9781     31801\n           1     0.9446    0.9318    0.9382     11370\n\n    accuracy                         0.9677     43171\n   macro avg     0.9602    0.9562    0.9581     43171\nweighted avg     0.9676    0.9677    0.9676     43171\n\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 4/10 [04:55<07:22, 73.82s/it]","output_type":"stream"},{"name":"stdout","text":"------------------Test Result----------------------------\nTesting loss : 0.5532641410827637 | F1_score : 0.7023480662983426\nConfusion matrix :\n[[3503  435]\n [ 427 1017]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8913    0.8895    0.8904      3938\n           1     0.7004    0.7043    0.7023      1444\n\n    accuracy                         0.8398      5382\n   macro avg     0.7959    0.7969    0.7964      5382\nweighted avg     0.8401    0.8398    0.8400      5382\n\n---------------------------------------------------------\nEpoch 4=======================================\n------------------Train Result----------------------------\nTraining loss : 0.04871148243546486 | F1_score : 0.9668973420901837\nConfusion matrix :\n[[31452   349]\n [  402 10968]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.9874    0.9890    0.9882     31801\n           1     0.9692    0.9646    0.9669     11370\n\n    accuracy                         0.9826     43171\n   macro avg     0.9783    0.9768    0.9775     43171\nweighted avg     0.9826    0.9826    0.9826     43171\n\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 5/10 [06:08<06:08, 73.71s/it]","output_type":"stream"},{"name":"stdout","text":"------------------Test Result----------------------------\nTesting loss : 0.6766282916069031 | F1_score : 0.6851457000710732\nConfusion matrix :\n[[3532  406]\n [ 480  964]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8804    0.8969    0.8886      3938\n           1     0.7036    0.6676    0.6851      1444\n\n    accuracy                         0.8354      5382\n   macro avg     0.7920    0.7822    0.7868      5382\nweighted avg     0.8329    0.8354    0.8340      5382\n\n---------------------------------------------------------\nEpoch 5=======================================\n------------------Train Result----------------------------\nTraining loss : 0.0291376281529665 | F1_score : 0.9817175002197415\nConfusion matrix :\n[[31586   215]\n [  201 11169]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.9937    0.9932    0.9935     31801\n           1     0.9811    0.9823    0.9817     11370\n\n    accuracy                         0.9904     43171\n   macro avg     0.9874    0.9878    0.9876     43171\nweighted avg     0.9904    0.9904    0.9904     43171\n\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 6/10 [07:22<04:54, 73.66s/it]","output_type":"stream"},{"name":"stdout","text":"------------------Test Result----------------------------\nTesting loss : 0.7663545608520508 | F1_score : 0.6924453992123164\nConfusion matrix :\n[[3556  382]\n [ 477  967]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8817    0.9030    0.8922      3938\n           1     0.7168    0.6697    0.6924      1444\n\n    accuracy                         0.8404      5382\n   macro avg     0.7993    0.7863    0.7923      5382\nweighted avg     0.8375    0.8404    0.8386      5382\n\n---------------------------------------------------------\nEpoch 6=======================================\n------------------Train Result----------------------------\nTraining loss : 0.020146364346146584 | F1_score : 0.9870278351875468\nConfusion matrix :\n[[31653   148]\n [  147 11223]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.9954    0.9953    0.9954     31801\n           1     0.9870    0.9871    0.9870     11370\n\n    accuracy                         0.9932     43171\n   macro avg     0.9912    0.9912    0.9912     43171\nweighted avg     0.9932    0.9932    0.9932     43171\n\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 7/10 [08:35<03:40, 73.60s/it]","output_type":"stream"},{"name":"stdout","text":"------------------Test Result----------------------------\nTesting loss : 0.8163372874259949 | F1_score : 0.7046109510086456\nConfusion matrix :\n[[3584  354]\n [ 466  978]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8849    0.9101    0.8973      3938\n           1     0.7342    0.6773    0.7046      1444\n\n    accuracy                         0.8476      5382\n   macro avg     0.8096    0.7937    0.8010      5382\nweighted avg     0.8445    0.8476    0.8456      5382\n\n---------------------------------------------------------\nEpoch 7=======================================\n------------------Train Result----------------------------\nTraining loss : 0.01708260178565979 | F1_score : 0.989528335093277\nConfusion matrix :\n[[31688   113]\n [  125 11245]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.9961    0.9964    0.9963     31801\n           1     0.9901    0.9890    0.9895     11370\n\n    accuracy                         0.9945     43171\n   macro avg     0.9931    0.9927    0.9929     43171\nweighted avg     0.9945    0.9945    0.9945     43171\n\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 8/10 [09:49<02:27, 73.57s/it]","output_type":"stream"},{"name":"stdout","text":"------------------Test Result----------------------------\nTesting loss : 0.8887156248092651 | F1_score : 0.6957142857142857\nConfusion matrix :\n[[3556  382]\n [ 470  974]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8833    0.9030    0.8930      3938\n           1     0.7183    0.6745    0.6957      1444\n\n    accuracy                         0.8417      5382\n   macro avg     0.8008    0.7888    0.7944      5382\nweighted avg     0.8390    0.8417    0.8401      5382\n\n---------------------------------------------------------\nEpoch 8=======================================\n------------------Train Result----------------------------\nTraining loss : 0.016425568610429764 | F1_score : 0.9886068710684907\nConfusion matrix :\n[[31675   126]\n [  133 11237]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.9958    0.9960    0.9959     31801\n           1     0.9889    0.9883    0.9886     11370\n\n    accuracy                         0.9940     43171\n   macro avg     0.9924    0.9922    0.9923     43171\nweighted avg     0.9940    0.9940    0.9940     43171\n\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 9/10 [11:02<01:13, 73.55s/it]","output_type":"stream"},{"name":"stdout","text":"------------------Test Result----------------------------\nTesting loss : 0.84549880027771 | F1_score : 0.6942034015966678\nConfusion matrix :\n[[3501  437]\n [ 444 1000]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8875    0.8890    0.8882      3938\n           1     0.6959    0.6925    0.6942      1444\n\n    accuracy                         0.8363      5382\n   macro avg     0.7917    0.7908    0.7912      5382\nweighted avg     0.8361    0.8363    0.8362      5382\n\n---------------------------------------------------------\nEpoch 9=======================================\n------------------Train Result----------------------------\nTraining loss : 0.015145350247621536 | F1_score : 0.9906229363856482\nConfusion matrix :\n[[31707    94]\n [  119 11251]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.9963    0.9970    0.9967     31801\n           1     0.9917    0.9895    0.9906     11370\n\n    accuracy                         0.9951     43171\n   macro avg     0.9940    0.9933    0.9936     43171\nweighted avg     0.9951    0.9951    0.9951     43171\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [12:16<00:00, 73.65s/it]","output_type":"stream"},{"name":"stdout","text":"------------------Test Result----------------------------\nTesting loss : 0.9287515878677368 | F1_score : 0.696597685022799\nConfusion matrix :\n[[3524  414]\n [ 451  993]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8865    0.8949    0.8907      3938\n           1     0.7058    0.6877    0.6966      1444\n\n    accuracy                         0.8393      5382\n   macro avg     0.7961    0.7913    0.7936      5382\nweighted avg     0.8380    0.8393    0.8386      5382\n\n---------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"model = LSTMmodel(matrix_size).to(device)\nmodel.load_state_dict(torch.load('best_LSTMmodel.pth'))\ntest_step(model,test_dataloader,BCE_loss,Adam_optimizer,device = device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T16:12:19.537931Z","iopub.execute_input":"2024-12-13T16:12:19.538398Z","iopub.status.idle":"2024-12-13T16:12:24.201184Z","shell.execute_reply.started":"2024-12-13T16:12:19.538369Z","shell.execute_reply":"2024-12-13T16:12:24.200310Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/4028531333.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_LSTMmodel.pth'))\n","output_type":"stream"},{"name":"stdout","text":"------------------Test Result----------------------------\nTesting loss : 0.37176916003227234 | F1_score : 0.7247706422018348\nUpdate new best model to : /kaggle/working/best_LSTMmodel.pth\nConfusion matrix :\n[[3575  377]\n [ 403 1027]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8987    0.9046    0.9016      3952\n           1     0.7315    0.7182    0.7248      1430\n\n    accuracy                         0.8551      5382\n   macro avg     0.8151    0.8114    0.8132      5382\nweighted avg     0.8543    0.8551    0.8546      5382\n\n---------------------------------------------------------\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch.nn.functional as F\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim):\n        super(Attention, self).__init__()\n        self.attention = nn.Linear(hidden_dim, 1)\n\n    def forward(self, lstm_output):\n        \"\"\"\n        lstm_output: Tensor of shape (batch_size, seq_len, hidden_dim)\n        \"\"\"\n        # Compute attention scores\n        attn_scores = self.attention(lstm_output).squeeze(-1)  # (batch_size, seq_len)\n        attn_weights = F.softmax(attn_scores, dim=1)  # Normalize scores to probabilities\n        \n        # Compute context vector as weighted sum of LSTM outputs\n        context = torch.bmm(attn_weights.unsqueeze(1), lstm_output).squeeze(1)  # (batch_size, hidden_dim)\n        \n        return context, attn_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T16:33:00.463680Z","iopub.execute_input":"2024-12-13T16:33:00.464036Z","iopub.status.idle":"2024-12-13T16:33:00.470012Z","shell.execute_reply.started":"2024-12-13T16:33:00.464004Z","shell.execute_reply":"2024-12-13T16:33:00.469157Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\n\ndef F1_tensor(y_true, y_pred):\n    y_true = y_true.to('cpu').numpy()\n    y_pred = y_pred.to('cpu').numpy()\n    return f1_score(y_true, y_pred)\n\ndef Confusion_matrix_tensor(y_true, y_pred):\n    y_true = y_true.to('cpu').numpy()\n    y_pred = y_pred.to('cpu').numpy()\n    return f1_score(y_true, y_pred)\n\ndef convert_from_tensor(y): #convert from tensor to some kind of array that we can use numpy\n    return y.cpu().detach().numpy().reshape(-1)\n\ndef take_all_elem(container, target):\n    for x in target:\n        if (x != 0 and x != 1):\n            container.append(1)\n        else:\n            container.append(x)\n\ndef save_model(model):\n    MODEL_PATH = Path('/content')\n    MODEL_PATH.mkdir(parents = True, exist_ok = True)\n    MODEL_NAME = 'best_GRUmodel.pth'\n    MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n    print(f'Update new best model to : {MODEL_SAVE_PATH}')\n    torch.save(obj = model.state_dict(),f = MODEL_SAVE_PATH)\n\n","metadata":{"id":"hy5vE5fPO5-m","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T16:33:02.932227Z","iopub.execute_input":"2024-12-13T16:33:02.932604Z","iopub.status.idle":"2024-12-13T16:33:02.939813Z","shell.execute_reply.started":"2024-12-13T16:33:02.932574Z","shell.execute_reply":"2024-12-13T16:33:02.938879Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def train_step(model : nn.Module,\n               data_loader : torch.utils.data.DataLoader,\n               loss_function : nn.Module,\n               optimizer,\n               device = 'cuda'):\n    model.train()\n    loss = 0\n\n    all_y_true = []\n    all_y_pred = []\n\n    for (X_train,y_train,mask) in data_loader:\n        X_train = X_train.to(device)\n        y_train = y_train.unsqueeze(1).to(device)\n\n        y_pred = model(X_train)\n        y_pred01 = torch.round(torch.sigmoid(y_pred))\n\n        batch_loss = loss_function(y_pred.float(),y_train.float())\n        loss += batch_loss\n\n        take_all_elem(all_y_true,convert_from_tensor(y_train))\n        take_all_elem(all_y_pred,convert_from_tensor(y_pred01))\n\n\n        optimizer.zero_grad()\n        batch_loss.backward()\n        optimizer.step()\n\n    loss /= len(data_loader)\n\n    all_y_true = np.array(all_y_true)\n    all_y_pred = np.array(all_y_pred)\n\n    #print(all_y_true)\n    #print(np.unique(all_y_true))\n\n    print('------------------Train Result----------------------------')\n    print(f'Training loss : {loss} | F1_score : {f1_score(all_y_true,all_y_pred)}')\n\n    print(classification_report(all_y_true, all_y_pred, digits=4))\n\nbest_f1_score = -1\n","metadata":{"id":"2JlNwITjPH9W","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T16:34:11.519031Z","iopub.execute_input":"2024-12-13T16:34:11.519691Z","iopub.status.idle":"2024-12-13T16:34:11.526726Z","shell.execute_reply.started":"2024-12-13T16:34:11.519656Z","shell.execute_reply":"2024-12-13T16:34:11.525703Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def test_step(model : nn.Module,\n              data_loader : torch.utils.data.DataLoader,\n              loss_function : nn.Module,\n              optimizer,\n              device = 'cuda',):\n\n    model.eval()\n    loss,acc = 0,0\n\n    all_y_true = []\n    all_y_pred = []\n\n    with torch.inference_mode():\n        loss = 0\n\n        for (X_test,y_test,mask) in data_loader:\n            X_test = X_test.to(device)\n            y_test = y_test.to(device)\n\n            test_logits = model(X_test).squeeze()\n            test_01 = torch.round(torch.sigmoid(test_logits))\n\n            batch_loss = loss_function(test_logits.float(),y_test.float())\n\n            loss += batch_loss\n\n            take_all_elem(all_y_true,convert_from_tensor(y_test))\n            take_all_elem(all_y_pred,convert_from_tensor(test_01))\n\n\n        loss /= len(data_loader)\n        acc /= len(data_loader)\n\n    current_f1_score = f1_score(all_y_true,all_y_pred)\n    print('------------------Test Result----------------------------')\n    print(f'Testing loss : {loss} | F1_score : {current_f1_score}')\n    print('---------------------------------------------------------')\n\n    print(classification_report(all_y_true, all_y_pred, digits=4))\n\n    global best_f1_score\n    if (current_f1_score > best_f1_score):\n        best_f1_score = current_f1_score\n        save_model(model)\n\nmatrix_size = (128,768)\n\n\n","metadata":{"id":"iu8piAnkPNa2","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T16:34:19.731801Z","iopub.execute_input":"2024-12-13T16:34:19.732427Z","iopub.status.idle":"2024-12-13T16:34:19.739556Z","shell.execute_reply.started":"2024-12-13T16:34:19.732390Z","shell.execute_reply":"2024-12-13T16:34:19.738654Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class GRUmodel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.embedding = nn.Embedding(64001, 768)\n        self.rnn = nn.GRU(input_size = matrix_size[1],hidden_size = 384,\n            num_layers = 3, batch_first = True, bidirectional = False)\n        self.attention = Attention(384)\n        self.fc = nn.Linear(384,out_features = 1)\n\n    #it output [0,1,2,....,seq_length - 1]\n    #just take the last array element in case of classification or anything like that\n    def forward(self, X, state=None):\n        X = self.embedding(X)\n        gru_outputs, _ = self.rnn(X, state)\n        context, attn_weights = self.attention(gru_outputs)\n        return self.fc(context)\n","metadata":{"id":"fzdMDVtAPQqp","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T16:34:26.247456Z","iopub.execute_input":"2024-12-13T16:34:26.247812Z","iopub.status.idle":"2024-12-13T16:34:26.253603Z","shell.execute_reply.started":"2024-12-13T16:34:26.247777Z","shell.execute_reply":"2024-12-13T16:34:26.252717Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel = GRUmodel().to(device)\nBCE_loss = nn.BCEWithLogitsLoss()\nAdam_optimizer = torch.optim.AdamW(params = model.parameters(),lr = 0.0001)\n","metadata":{"id":"Ps5B2GbBPS3G","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T16:34:27.966428Z","iopub.execute_input":"2024-12-13T16:34:27.967101Z","iopub.status.idle":"2024-12-13T16:34:28.549772Z","shell.execute_reply.started":"2024-12-13T16:34:27.967062Z","shell.execute_reply":"2024-12-13T16:34:28.549038Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"epochs = 10\n\nfor epoch in range(0,epochs):\n    print(f'Epoch {epoch}=======================================')\n    train_step(model,train_dataloader,BCE_loss,Adam_optimizer,device = device)\n    test_step( model,test_dataloader,BCE_loss,Adam_optimizer,device = device)\n    ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OTyy5lf3PUal","outputId":"834762b1-31af-4025-b5e7-74d6093a59d3","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T16:34:29.911539Z","iopub.execute_input":"2024-12-13T16:34:29.911877Z","iopub.status.idle":"2024-12-13T16:39:03.465061Z","shell.execute_reply.started":"2024-12-13T16:34:29.911845Z","shell.execute_reply":"2024-12-13T16:39:03.464278Z"}},"outputs":[{"name":"stdout","text":"Epoch 0=======================================\n------------------Train Result----------------------------\nTraining loss : 0.45532846450805664 | F1_score : 0.4922018620648757\n              precision    recall  f1-score   support\n\n           0     0.8084    0.9578    0.8768     31801\n           1     0.7555    0.3650    0.4922     11370\n\n    accuracy                         0.8016     43171\n   macro avg     0.7819    0.6614    0.6845     43171\nweighted avg     0.7945    0.8016    0.7755     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.3817872405052185 | F1_score : 0.6669282071400549\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8639    0.9319    0.8967      3952\n           1     0.7596    0.5944    0.6669      1430\n\n    accuracy                         0.8423      5382\n   macro avg     0.8118    0.7632    0.7818      5382\nweighted avg     0.8362    0.8423    0.8356      5382\n\nUpdate new best model to : /content/best_GRUmodel.pth\nEpoch 1=======================================\n------------------Train Result----------------------------\nTraining loss : 0.3503006398677826 | F1_score : 0.684590490415733\n              precision    recall  f1-score   support\n\n           0     0.8695    0.9421    0.9044     31801\n           1     0.7889    0.6047    0.6846     11370\n\n    accuracy                         0.8533     43171\n   macro avg     0.8292    0.7734    0.7945     43171\nweighted avg     0.8483    0.8533    0.8465     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.368364155292511 | F1_score : 0.6829463570856685\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8662    0.9456    0.9042      3952\n           1     0.7987    0.5965    0.6829      1430\n\n    accuracy                         0.8528      5382\n   macro avg     0.8325    0.7711    0.7936      5382\nweighted avg     0.8483    0.8528    0.8454      5382\n\nUpdate new best model to : /content/best_GRUmodel.pth\nEpoch 2=======================================\n------------------Train Result----------------------------\nTraining loss : 0.30131620168685913 | F1_score : 0.7417692527912968\n              precision    recall  f1-score   support\n\n           0     0.8929    0.9429    0.9172     31801\n           1     0.8107    0.6836    0.7418     11370\n\n    accuracy                         0.8746     43171\n   macro avg     0.8518    0.8133    0.8295     43171\nweighted avg     0.8712    0.8746    0.8710     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.34692099690437317 | F1_score : 0.7104660045836515\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8808    0.9347    0.9069      3952\n           1     0.7828    0.6503    0.7105      1430\n\n    accuracy                         0.8592      5382\n   macro avg     0.8318    0.7925    0.8087      5382\nweighted avg     0.8548    0.8592    0.8547      5382\n\nUpdate new best model to : /content/best_GRUmodel.pth\nEpoch 3=======================================\n------------------Train Result----------------------------\nTraining loss : 0.2573848366737366 | F1_score : 0.7905364816782957\n              precision    recall  f1-score   support\n\n           0     0.9144    0.9460    0.9299     31801\n           1     0.8328    0.7523    0.7905     11370\n\n    accuracy                         0.8950     43171\n   macro avg     0.8736    0.8492    0.8602     43171\nweighted avg     0.8929    0.8950    0.8932     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.34449970722198486 | F1_score : 0.6994797919167667\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8711    0.9507    0.9091      3952\n           1     0.8176    0.6112    0.6995      1430\n\n    accuracy                         0.8605      5382\n   macro avg     0.8443    0.7809    0.8043      5382\nweighted avg     0.8569    0.8605    0.8534      5382\n\nEpoch 4=======================================\n------------------Train Result----------------------------\nTraining loss : 0.21317395567893982 | F1_score : 0.8377210930750694\n              precision    recall  f1-score   support\n\n           0     0.9337    0.9556    0.9445     31801\n           1     0.8672    0.8102    0.8377     11370\n\n    accuracy                         0.9173     43171\n   macro avg     0.9004    0.8829    0.8911     43171\nweighted avg     0.9162    0.9173    0.9164     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.37564602494239807 | F1_score : 0.7151304347826087\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8979    0.8945    0.8962      3952\n           1     0.7114    0.7189    0.7151      1430\n\n    accuracy                         0.8478      5382\n   macro avg     0.8047    0.8067    0.8057      5382\nweighted avg     0.8483    0.8478    0.8481      5382\n\nUpdate new best model to : /content/best_GRUmodel.pth\nEpoch 5=======================================\n------------------Train Result----------------------------\nTraining loss : 0.16513261198997498 | F1_score : 0.877806601663834\n              precision    recall  f1-score   support\n\n           0     0.9516    0.9631    0.9573     31801\n           1     0.8931    0.8631    0.8778     11370\n\n    accuracy                         0.9367     43171\n   macro avg     0.9223    0.9131    0.9176     43171\nweighted avg     0.9362    0.9367    0.9364     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.4207930862903595 | F1_score : 0.7018224117875146\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8760    0.9383    0.9060      3952\n           1     0.7876    0.6329    0.7018      1430\n\n    accuracy                         0.8571      5382\n   macro avg     0.8318    0.7856    0.8039      5382\nweighted avg     0.8525    0.8571    0.8518      5382\n\nEpoch 6=======================================\n------------------Train Result----------------------------\nTraining loss : 0.12299606949090958 | F1_score : 0.9146833311173159\n              precision    recall  f1-score   support\n\n           0     0.9671    0.9725    0.9698     31801\n           1     0.9219    0.9076    0.9147     11370\n\n    accuracy                         0.9554     43171\n   macro avg     0.9445    0.9400    0.9423     43171\nweighted avg     0.9552    0.9554    0.9553     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.47103145718574524 | F1_score : 0.7148536320925191\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8910    0.9119    0.9013      3952\n           1     0.7397    0.6916    0.7149      1430\n\n    accuracy                         0.8534      5382\n   macro avg     0.8153    0.8018    0.8081      5382\nweighted avg     0.8508    0.8534    0.8518      5382\n\nEpoch 7=======================================\n------------------Train Result----------------------------\nTraining loss : 0.09198715537786484 | F1_score : 0.9388422448077772\n              precision    recall  f1-score   support\n\n           0     0.9766    0.9800    0.9783     31801\n           1     0.9434    0.9343    0.9388     11370\n\n    accuracy                         0.9679     43171\n   macro avg     0.9600    0.9571    0.9586     43171\nweighted avg     0.9679    0.9679    0.9679     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.4784201681613922 | F1_score : 0.7099152230003687\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8861    0.9190    0.9022      3952\n           1     0.7506    0.6734    0.7099      1430\n\n    accuracy                         0.8538      5382\n   macro avg     0.8183    0.7962    0.8061      5382\nweighted avg     0.8501    0.8538    0.8511      5382\n\nEpoch 8=======================================\n------------------Train Result----------------------------\nTraining loss : 0.06715718656778336 | F1_score : 0.9567438992159283\n              precision    recall  f1-score   support\n\n           0     0.9840    0.9852    0.9846     31801\n           1     0.9583    0.9551    0.9567     11370\n\n    accuracy                         0.9773     43171\n   macro avg     0.9712    0.9702    0.9707     43171\nweighted avg     0.9772    0.9773    0.9772     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.5705175399780273 | F1_score : 0.7068513665293897\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8826    0.9248    0.9032      3952\n           1     0.7607    0.6601    0.7069      1430\n\n    accuracy                         0.8545      5382\n   macro avg     0.8217    0.7925    0.8051      5382\nweighted avg     0.8502    0.8545    0.8511      5382\n\nEpoch 9=======================================\n------------------Train Result----------------------------\nTraining loss : 0.05251249670982361 | F1_score : 0.966088258610059\n              precision    recall  f1-score   support\n\n           0     0.9874    0.9884    0.9879     31801\n           1     0.9675    0.9646    0.9661     11370\n\n    accuracy                         0.9822     43171\n   macro avg     0.9775    0.9765    0.9770     43171\nweighted avg     0.9821    0.9822    0.9822     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.5913235545158386 | F1_score : 0.7070707070707071\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8828    0.9246    0.9032      3952\n           1     0.7603    0.6608    0.7071      1430\n\n    accuracy                         0.8545      5382\n   macro avg     0.8215    0.7927    0.8051      5382\nweighted avg     0.8503    0.8545    0.8511      5382\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"test_step(model,test_dataloader,BCE_loss,Adam_optimizer,device = device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T16:47:43.977917Z","iopub.execute_input":"2024-12-13T16:47:43.978296Z","iopub.status.idle":"2024-12-13T16:47:45.097114Z","shell.execute_reply.started":"2024-12-13T16:47:43.978251Z","shell.execute_reply":"2024-12-13T16:47:45.096281Z"}},"outputs":[{"name":"stdout","text":"------------------Test Result----------------------------\nTesting loss : 0.5913235545158386 | F1_score : 0.7070707070707071\n---------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0     0.8828    0.9246    0.9032      3952\n           1     0.7603    0.6608    0.7071      1430\n\n    accuracy                         0.8545      5382\n   macro avg     0.8215    0.7927    0.8051      5382\nweighted avg     0.8503    0.8545    0.8511      5382\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"class BCNN(nn.Module):\n    def __init__(self):\n\n        super().__init__()\n\n        self.embedding = nn.Embedding(64001,768)\n        self.bidirectional_lstm = nn.LSTM(\n            768, 512, 3,bidirectional=True, batch_first=True\n        )\n        self.conv1 = nn.Conv1d(in_channels=2*512, out_channels=32, kernel_size=4)\n        self.conv2 = nn.Conv1d(in_channels=2*512, out_channels=32, kernel_size=5)\n        self.attention = Attention(64)\n        self.fc = nn.Linear(64, 1)\n\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, X):\n\n        #embedded = [batch size, sent len, emb dim]\n        #embedded = self.fc_input(encoded)\n        #print(embedded.shape)\n        encoded = self.embedding(X)\n        embedded, _ = self.bidirectional_lstm(encoded)\n        embedded = embedded.permute(0, 2, 1)\n        #print(embedded.shape)\n        conved_1 = F.relu(self.conv1(embedded))\n        conved_2 = F.relu(self.conv2(embedded))\n        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n\n        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n        #pooled_n = [batch size, n_fibatlters]\n\n        cat = self.dropout(torch.cat((pooled_1, pooled_2), dim = -1))\n\n        #cat = [batch size, n_filters * len(filter_sizes)]\n\n        context, _ = self.attention(cat.unsqueeze(1))\n        \n        result =  self.fc(cat)\n\n        return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:47:08.967587Z","iopub.execute_input":"2024-12-11T15:47:08.968483Z","iopub.status.idle":"2024-12-11T15:47:08.975937Z","shell.execute_reply.started":"2024-12-11T15:47:08.968446Z","shell.execute_reply":"2024-12-11T15:47:08.974902Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"device = 'cuda'\nmodel = BCNN().to(device)\nBCE_loss = nn.BCEWithLogitsLoss()\nAdam_optimizer = torch.optim.AdamW(params = model.parameters(),lr = 0.001)\n\n\nepochs = 3\nbest_f1_score = -1\nfor epoch in tqdm(range(0,epochs)):\n    print(f'Epoch {epoch}=======================================')\n    train_step(model,train_dataloader,BCE_loss,Adam_optimizer,device = device)\n    test_step(model,val_dataloader,BCE_loss,Adam_optimizer,device = device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:51:02.699856Z","iopub.execute_input":"2024-12-11T15:51:02.700844Z","iopub.status.idle":"2024-12-11T15:57:00.209127Z","shell.execute_reply.started":"2024-12-11T15:51:02.700805Z","shell.execute_reply":"2024-12-11T15:57:00.208246Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/3 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 0=======================================\n------------------Train Result----------------------------\nTraining loss : 0.4337882697582245 | F1_score : 0.5533884297520661\nConfusion matrix :\n[[30043  1758]\n [ 6348  5022]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8256    0.9447    0.8811     31801\n           1     0.7407    0.4417    0.5534     11370\n\n    accuracy                         0.8122     43171\n   macro avg     0.7831    0.6932    0.7173     43171\nweighted avg     0.8032    0.8122    0.7948     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.37103307247161865 | F1_score : 0.6644924582144313\nUpdate new best model to : /kaggle/working/best_LSTMmodel.pth\n","output_type":"stream"},{"name":"stderr","text":" 33%|███▎      | 1/3 [01:59<03:58, 119.04s/it]","output_type":"stream"},{"name":"stdout","text":"Confusion matrix :\n[[3744  194]\n [ 629  815]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8562    0.9507    0.9010      3938\n           1     0.8077    0.5644    0.6645      1444\n\n    accuracy                         0.8471      5382\n   macro avg     0.8319    0.7576    0.7827      5382\nweighted avg     0.8432    0.8471    0.8375      5382\n\n---------------------------------------------------------\nEpoch 1=======================================\n------------------Train Result----------------------------\nTraining loss : 0.3019334673881531 | F1_score : 0.7422858790733441\nConfusion matrix :\n[[30087  1714]\n [ 3648  7722]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8919    0.9461    0.9182     31801\n           1     0.8184    0.6792    0.7423     11370\n\n    accuracy                         0.8758     43171\n   macro avg     0.8551    0.8126    0.8302     43171\nweighted avg     0.8725    0.8758    0.8719     43171\n\n------------------Test Result----------------------------\nTesting loss : 0.36201488971710205 | F1_score : 0.7219061166429587\nUpdate new best model to : /kaggle/working/best_LSTMmodel.pth\n","output_type":"stream"},{"name":"stderr","text":" 67%|██████▋   | 2/3 [03:58<01:59, 119.06s/it]","output_type":"stream"},{"name":"stdout","text":"Confusion matrix :\n[[3585  353]\n [ 429 1015]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8931    0.9104    0.9017      3938\n           1     0.7420    0.7029    0.7219      1444\n\n    accuracy                         0.8547      5382\n   macro avg     0.8175    0.8066    0.8118      5382\nweighted avg     0.8526    0.8547    0.8534      5382\n\n---------------------------------------------------------\nEpoch 2=======================================\n------------------Train Result----------------------------\nTraining loss : 0.21027088165283203 | F1_score : 0.8368522072936659\nConfusion matrix :\n[[30445  1356]\n [ 2214  9156]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.9322    0.9574    0.9446     31801\n           1     0.8710    0.8053    0.8369     11370\n\n    accuracy                         0.9173     43171\n   macro avg     0.9016    0.8813    0.8907     43171\nweighted avg     0.9161    0.9173    0.9162     43171\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3/3 [05:56<00:00, 118.92s/it]","output_type":"stream"},{"name":"stdout","text":"------------------Test Result----------------------------\nTesting loss : 0.391086608171463 | F1_score : 0.7038012796386902\nConfusion matrix :\n[[3660  278]\n [ 509  935]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8779    0.9294    0.9029      3938\n           1     0.7708    0.6475    0.7038      1444\n\n    accuracy                         0.8538      5382\n   macro avg     0.8244    0.7885    0.8034      5382\nweighted avg     0.8492    0.8538    0.8495      5382\n\n---------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"model = BCNN().to(device)\nmodel.load_state_dict(torch.load('best_LSTMmodel.pth'))\ntest_step(model,test_dataloader,BCE_loss,Adam_optimizer,device = device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:59:31.937639Z","iopub.execute_input":"2024-12-11T15:59:31.938357Z","iopub.status.idle":"2024-12-11T15:59:38.724687Z","shell.execute_reply.started":"2024-12-11T15:59:31.938315Z","shell.execute_reply":"2024-12-11T15:59:38.723740Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_503/1568864093.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_LSTMmodel.pth'))\n","output_type":"stream"},{"name":"stdout","text":"------------------Test Result----------------------------\nTesting loss : 0.3403221368789673 | F1_score : 0.7262206148282097\nUpdate new best model to : /kaggle/working/best_LSTMmodel.pth\nConfusion matrix :\n[[3621  331]\n [ 426 1004]]\nClassification report :\n              precision    recall  f1-score   support\n\n           0     0.8947    0.9162    0.9054      3952\n           1     0.7521    0.7021    0.7262      1430\n\n    accuracy                         0.8593      5382\n   macro avg     0.8234    0.8092    0.8158      5382\nweighted avg     0.8568    0.8593    0.8578      5382\n\n---------------------------------------------------------\n","output_type":"stream"}],"execution_count":8}]}